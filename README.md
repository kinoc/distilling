# distilling
Experiments with distilling large language models.

# STEPS
1. Use create_data.py to create a pickle file of containg the list of input_ids of the text corpus.
2. Use run_distillation.py to start distillation training.
